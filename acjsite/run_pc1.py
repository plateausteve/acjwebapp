"""Run Bayesian analysis of data from a paired-comparison experiment.
This script should be used as a template,
to be copied and modified for any particular experiment.

*** Usage, four main steps, see also explicit template example below

*1: Create a PairedCompFrame instance to define experiment and select input data.

*2: Load a set of test results

*3: Learn Bayesian model corresponding to observed data
using either Thurstone Case V or Bradley-Terry-Luce probabilistic choice model.

*4: Display results and save figures and tables to a directory tree
"""

from pathlib import Path
import pickle
import logging

from PairedCompCalc import pc_logging, __version__
from PairedCompCalc.pc_data import PairedCompFrame, PairedCompDataSet
from PairedCompCalc.pc_model import PairedCompResultSet, Thurstone, Bradley

import PairedCompCalc.pc_display as shw

# -----------------------------------
# model_class = Bradley
model_class = Thurstone
# = selected class of probabilistic choice model for the analysis

# ---------------------- location of input and output data:
work_path = Path.home() / 'Documents' / 'My_PairedComp_project'  # or whatever...

data_path = work_path / 'data'
# = directory to be searched for read-only input data files

result_path = work_path / 'result'
# = top directory for all result files,
# with sub-directories created as needed.
# NOTE: Existing result files in this directory are OVER-WRITTEN WITHOUT WARNING,
# but different analysis runs may add new result files to the same existing directory.

assert result_path != data_path, 'Result directory must be different from input data directory'

model_result_file = 'pc_result.pkl'
# = name of file with saved PairedCompResultSet instance, if used

display_file = 'pc_displays.pkl'
# = name of file with saved PairedCompDisplaySet instance, if used

log_file = 'run_pc_log.txt'
# = name of log file

result_path.mkdir(parents=True, exist_ok=True)

pc_logging.setup(result_path, log_file)
logging.info(f'*** Running PairedCompCalc version {__version__}')

# ---------------------- Define experimental structure:

# NOTE: all string values are CASE-sensitive,
# e.g., objects 'A' and 'a' are DIFFERENT,

# pcf = PairedCompFrame(attributes=['Speech Clarity', 'Pleasantness', 'Preference'],
#                       objects=['Program0', 'Program1', 'Program2'], # may be taken from input data
#                       objects_alias=['A', 'B', 'C', 'D'],  # to hide real object names in displays
#                       forced_choice=False,
#                       difference_grades=['Equal', 'Slightly Better', 'Better', 'Much Better'],
#                       test_factors={'Background': ['Quiet', 'Noisy']}
#                       )

# Test example for data generated by run_sim.py:
pcf = PairedCompFrame(attributes=['SimQ'],
                      # objects=[f'HA{i}' for i in range(3)],  # accepting all from input data file
                      forced_choice=False,
                      difference_grades=['Equal', 'Slightly Better', 'Better', 'Much Better'],
                      test_factors={'Stim': ['speech', 'music'], 'SNR': ['Quiet', 'High']}
                      )


# ---------------------- Main analysis work:

logging.info(f'Analysing paired-comparison data in {data_path}')

# Using all xlsx data files in data_path and its sub-directories, for example:

# ds = PairedCompDataSet.load(pcf, data_path,
#                             fmt='xlsx',
#                             groups=['A', 'B'],
#                             sheets=[f'Subject{i}' for i in range(10)],
#                             subject='sheet',
#                             top_row=2,
#                             attribute='A',  # column with attribute label
#                             pair=('B', 'C'),  # columns with labels of presented object pair
#                             difference='D',  # column with difference_grade label
#                             choice='E',  # column with selected object
#                             no_choice=['None', ''],  # labels indicating no response, or no preference
#                             Sound='G'  # column with category of test-factor Sound
#                             )
# *** See pc_file_xlsx for more details about the xlsx input file format

# OR, using files formatted as pc_file_json.PairedCompFile:
# e.g., data files generated by run_sim.py:
ds = PairedCompDataSet.load(pcf, data_path, groups=['Group0'], fmt='json')

logging.info(f'Learning Results with model {model_class}')

pc_result = PairedCompResultSet.learn(ds, rv_class=model_class)

# ------------------------------- Optionally, dump learned result set:
with (result_path / model_result_file).open('wb') as f:
    pickle.dump(pc_result, f)

# may be re-loaded to save learning time,
# in case different display formats are desired later.

# ------------------------------- Generate result displays:

pc_display_set = shw.display(pc_result)
# = default display combination, showing estimated results for
# (1) random individual in the population from which participants were recruited,
# (2) the population mean.

# OR, for example:

# pc_display_set = shw.display(pc_result,
#                          percentiles=[2.5, 50., 97.5],
#                          credibility_limit=0.8,
#                          show_intervals=False,
#                          figure_format='pdf',
#                          table_format='tab')

# *** See pc_display.FMT and pc_display_format.FMT for available format parameters

# -------------------------------------------------------------------
# Alternative: display other combinations of predictive distributions:

# logging.info('Displaying predictive_group_individual and predictive_population_individual')
# pc_display_set = shw.PairedCompDisplaySet.display(pc_result.predictive_group_individual(),
#                                               pc_result.predictive_population_individual())

# OR
# logging.info('Displaying predictive_group_individual and predictive_population_mean')
# pc_display_set = shw.PairedCompDisplaySet.display(pc_result.predictive_group_individual(),
#                                               pc_result.predictive_population_mean())

# OR
# logging.info('Displaying only predictive_group_individual')
# pc_display_set = shw.PairedCompDisplaySet.display(pc_result.predictive_group_individual())

# OR other single predictive distribution or pair of predictive distributions.

# ***** Edit display plots or tables here, if needed *****
# Each display element can be accessed and modified by the user, before saving,

# plt.show()
# to show all figures on screen before saving,
# NOTE: This blocks the program until figure windows are closed by user

# -------------- Optional Likelihood Ratio significance test:

# NOTE: Training the NULL model takes same time as the actual model.
# Likelihood-Ratio p-value may not be reliable.
# Safer to use the credibility values shown in pc_display_set tables.

# logging.info(f'Running Likelihood Ratio Significance Test. ' +
#              'Null Hypothesis: Population mean quality parameters all equal.')
# logging.info('*** NOTE: Likelihood-Ratio p-values are approximate ***')
# logging.info(f'Learning Null-hypothesis model with {model_class}')
#
# pc_result_null = PairedCompResultSet.learn(ds, rv_class=model_class, null_quality=True)
# pc_display_set.likelihood_ratio_test(pc_result_null, pc_result)


# ------------------------------- save all result displays:
pc_display_set.save(result_path)

# Optionally, save the display set as a single editable object:
# if display_file is not None:
#     with (result_path / display_file).open('wb') as f:
#         pickle.dump(pc_display_set, f)

# *** can be loaded again, elements edited, and re-saved

logging.info(f'All results saved in {result_path} and sub-dirs.')
logging.shutdown()
